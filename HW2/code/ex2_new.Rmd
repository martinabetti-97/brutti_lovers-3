---
title: "Exercise 2"
author: "Stefano D'Arrigo & Martina Betti"
date: "5/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)

library(Matrix)
```

## Look into the data

Load the data

```{r eval=FALSE, include=FALSE}
load("../data/amazon_review_clean.RData")
```

The Xâ€“matrices are known as term-frequency matrices or document-term matrices and count how often various words are used in the text. For more info have also a look at the Kernel for Text slide set (pp. 142-147).
Specifically, the X[i,j] element counts the number of times the word wj appears in the ith review for some pre-specified set of words of interest. Let's check:

```{r eval=FALSE, include=FALSE}
dim(X_tr)
colnames(X_tr)[1:10]
round(as.matrix(X_tr[1:10, 1:10]), 2)
```


```{r}
X_tr <- as(X_tr, "sparseMatrix")
```

```{r}
X_te <- as(X_te, "sparseMatrix")
```


Implement a linear classifier: 

```{r}
# Define the regression function
f_reg <- function(X, beta) cbind(1,X) %*% beta

# Define the linear classifier
linear_classifier <- function(X, beta) ifelse(f_reg(X, beta) > 0.5, 1, 0)

# Define the gradient for the linear
gradient_linear <- function(X.T_X, X.T_Y, beta) X.T_X %*% beta - X.T_Y

# Define the empirical risk
empirical_risk <- function(y.pred.reg, y.true) mean((y.pred.reg-y.true)^2) # OLS

# Define the gradient descent function
gradient_descent <- function(X, Y, G, alpha=0.5, max_iter=100, tol=0.000001, beta=NULL, plot=FALSE) {
  ###
  # Function to compute the gradient descent given the gradient function G.
  # :param X : design matrix (n_observations * n_features)
  # :param Y : ground-truth labels
  # :param G : gradient function
  # :param alpha : learning rate
  # :param max_iter : maximum number of iterations; first stop condition
  # :param tol : tolerance; stop the gradient loop before max_iter is reached if the condition is met
  # :param beta : vector of parameters; provide a custom starting vector; otherwise, a random one is chosen
  # :param plot : boolean; plot the empirical risk history if TRUE 
  # :param threads : integer that defines the number of cores to use for the matrix precomputation
  #
  # :return list(beta=[vector of parameters], emp_risk_hist=[empirical risk through the iterations]) 
  ###
  
  # gradient = 2/n (X.T*X*beta - X.T*Y)
  
  n <- dim(X)[1] # number of observations (data points)
  d <- dim(X)[2] # number of features
  
  if(is.null(beta))
    beta <- matrix(rep(0,d+1))
  
  emp_risk_hist <- rep(0, max_iter+1)
  
  print("Precomputing the matrices")
  
  X.T_X <- crossprod(cbind(1,X)) * (2 / n) # (d+1) x (d+1); equivalent to t(X)%*%X but faster
  X.T_Y <- crossprod(cbind(1,X), Y) * (2 / n) # (d+1) x 1; 
  
  print("Entering gradient loop...")  
  
  # Gradient loop
  for(i in 1:max_iter) {

    grad <- G(X.T_X, X.T_Y, beta)
    
    # Update beta
    beta <- beta - alpha * grad # update rule

    # Compute and store the ER value of the i-th iteration
    emp_risk_hist[i+1] <- empirical_risk(f_reg(X, beta), Y)
    
    print(paste("Iteration", i, " ER:", emp_risk_hist[i+1]))
    
    # Check convergence
    if(abs(emp_risk_hist[i]-emp_risk_hist[i+1]) <= tol) {
      max_iter <- i
      break
    }
    
  }
  print(paste("Converged after", max_iter, "iterations"))

  if(plot)
    plot(seq(1,max_iter), emp_risk_hist[-1], col="blue",
         xlab="iter", ylab="ER", main="Gradient descent")

  return(list(param=beta, emp_risk_hist=emp_risk_hist[-1]))
}

```



Map the Y string labels into \{0,1\}:

```{r}
encode <- function(Y) {
  unique_labs <- unique(Y)
  Y_encoded <- as.integer(Y == unique_labs[2])
  return(matrix(Y_encoded))
}
```

Take a small random subset of the train set:

```{r eval=FALSE, include=FALSE}
n <- length(y_tr)
idxs <- sample(seq(1, n), size = n, replace = FALSE)
```

Encode the labels and compute the parameters of the trained model:

```{r eval=FALSE, include=FALSE}
y_tr_encoded <- encode(y_tr)# encode(y_tr[idxs])

t_0 <- Sys.time()
t_0

# with lr=0.003 the ER diverges, with lr=0.001 the ER converges, but slowly
result <- gradient_descent(X_tr, y_tr_encoded, gradient_linear, alpha=0.001, max_iter=1000, plot=F)
Sys.time() - t_0
```

Take a small random subset of the test set:

```{r eval=FALSE, include=FALSE}
n <- length(y_te)
idxs <- sample(seq(1, n), size = 1000, replace = FALSE)
```

Encode the labels of the ground-truth vector and compute the predictions:

```{r eval=FALSE, include=FALSE}
y_te_encoded <- encode(y_te) # [idxs]

pred <- linear_classifier(X_te, result$param) # [idxs,]
```

Rapidly compute the number of right predictions:

```{r eval=FALSE, include=FALSE}
perf <- sum(pred == y_te_encoded) / length(y_te_encoded)
```

Check the number of right predictions:

```{r eval=FALSE, include=FALSE}
perf
```

Plot the ER:

```{r}
plot(seq(1,length(result$emp_risk_hist)), result$emp_risk_hist, col="blue",
         xlab="iter", ylab="ER", main="Gradient descent")
```

Keep training: after 3500 iterations with alpha=0.001, the accuracy on the entire test set is 93.9% (in about 10 minutes training).

```{r}
t_0 <- Sys.time()
t_0

# with lr=0.003 the ER diverges, with lr=0.001 the ER converges, but slowly
result <- gradient_descent(X_tr, y_tr_encoded, gradient_linear, alpha=0.001, max_iter=1000, beta=result$param, plot=F)
Sys.time() - t_0
```



```{r}
# Generate Random Data
n <- 1000
y <- sample(c(0,1), size=n, replace=TRUE, prob=c(.5,.5))
x_given_y <- c()
for (yi in y){
  if (yi==0){
    x <- runif(1, -3, 1)
  } else {
    x <- runif(1, -1, 3)
  }
  x_given_y <- c(x_given_y, x)
}
```

```{r}
x_tr.test <- matrix(x_given_y)
```

```{r}
result.test <- gradient_descent(x_tr.test, y, gradient_linear, alpha=0.03, max_iter=500, tol=0.001, plot=T)
```



```{r fig.width=18, fig.height=10}
require(viridis) 
library(DT)

# Generate Random Data
n <- 250
y <- sample(c(0,1), size=n, replace=TRUE, prob=c(.5,.5))
x_given_y <- c()
for (yi in y){
  if (yi==0){
    x <- runif(1, -3, 1)
  } else {
    x <- runif(1, -1, 3)
  }
  x_given_y <- c(x_given_y, x)
}


# Plot + Regression Function
mycol = viridis(7, alpha = .7)
plot(x = x_given_y, y = y, bg= ifelse(y == 1, mycol[1], mycol[7]),
     pch=21, xlab="X", ylab = "Y", main="Simulated Data Points + Linear Classifier", cex=3, 
     cex.lab=1.5, cex.axis=1.5, cex.main=2)
abline(h = 0.5, lty = 3, col = gray(.8))
abline(v = 0, lty = 3, col = gray(.8))
rug(x_given_y)
curve(linear_classifier(x, result.test$param), from=-3, to=3, add = TRUE, col="gray", lwd=3)
curve(f_reg(x, result.test$param), add=T)
```
```{r Loss Evaluator}
# Loss Function
loss <- function(y.actual, y.pred) {
  
  res = sum(y.actual != y.pred)
  return(res)
}

print(paste("Bayes Classifier Loss:", loss(y, linear_classifier(x_given_y, result.test$param))))
print(paste("Relative Bayes Classifier Loss:", loss(y, linear_classifier(x_given_y, result.test$param))/length(y)))

```


```{r fig.width=18, fig.height=10}
logistic_regression <- function(x, y) {
  
  data = data.frame(y, x)
  fit_logit <- glm(y ~ ., data = data, family = "binomial")
  params = coef(fit_logit)
  return(params)
}

logit_func <- function(x, parameters=params) 1/(1 + exp(-(parameters[1]+parameters[2]*x)))

params = logistic_regression(x=x_given_y, y=y)

# Plot + Regression Function
plot(x = x_given_y, y = y, bg= ifelse(y == 1, mycol[1], mycol[7]),
     pch=21, xlab="X", ylab = "Y", main="Simulated Data Points + Optimal Bayes Classifier", cex=3, 
     cex.lab=1.5, cex.axis=1.5, cex.main=2)
abline(h = 0.5, lty = 3, col = gray(.8))
abline(v = 0, lty = 3, col = gray(.8))
rug(x_given_y)
curve(linear_classifier(x, result.test$param), from=-3, to=3, add = TRUE, col="gray", lwd=3)
curve(logit_func(x, parameters=params), from=-3, to=3, add = TRUE, col="orchid", lwd=3)
curve(round(logit_func(x, parameters=params)), from=-3, to=3, add = TRUE, col="black", lwd=3)
legend(x=-3,, y=1, 
       legend=c("h Linear", "Logistic Regression", "h Opt Logistic Regression"), 
       #legend=c("1", "2", "3"), 
       col=c("gray", "orchid", "black"), cex=1.5, lty=1, lwd=3)

print(paste("Logistic Regression Classifier Loss:", loss(y, round(logit_func(x=x_given_y, parameters=params)))))
print(paste("Relative Logistic Regression Classifier Loss:", loss(y, round(logit_func(x=x_given_y, parameters=params)))/length(y)))
```

```{r}
true.f <- function(x) 2 * x + 3 + runif(n = length(x), min = -10, max = 10)

plot(true.f(seq(1,100)))
# lines(seq(1,100), cbind(1,matrix(seq(1,100))) %*% matrix(c(3,2), nrow=2))

lines(seq(1,100), f_reg(matrix(seq(1,100)), matrix(c(3,2), nrow=2)))
# r <- rep(0, 100)
# for(i in 1:100) {
#   r[i] <- 2*i + 3
# }
# 
# lines(seq(1,100), r, col="blue")
```


```{r}
x_tr <- sample(c(1:1000), size=500) / 1000
y_tr <- true.f(x_tr)

res <- gradient_descent(matrix(x_tr), matrix(y_tr), gradient_linear, alpha = 0.01, max_iter = 500, tol=0.001, plot = T)
```
```{r}
plot(x_tr, y_tr, ylim = c(-10, 50), xlim=c(0,1))
# lines(seq(1,100), cbind(1,matrix(seq(1,100))) %*% matrix(c(3,2), nrow=2))

lines(x_tr, f_reg(x_tr, res$param))
```



