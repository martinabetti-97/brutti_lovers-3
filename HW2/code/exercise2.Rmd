---
title: "Exercise 2"
author: "Stefano D'Arrigo & Martina Betti"
date: "5/21/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
set.seed(123)
library(purrr)
```

## Look into the data

Load the data

```{r}
load("../data/amazon_review_clean.RData")
```

The Xâ€“matrices are known as term-frequency matrices or document-term matrices and count how often various words are used in the text. For more info have also a look at the Kernel for Text slide set (pp. 142-147).
Specifically, the X[i,j] element counts the number of times the word wj appears in the ith review for some pre-specified set of words of interest. Let's check:

```{r}
dim(X_tr)
colnames(X_tr)[1:10]
round(as.matrix(X_tr[1:10, 1:10]), 2)
```


Implement a linear classifier: for simplicity, I've implemented the perceptron classifier, but another one can be used.

```{r}
# Define the perceptron classifier
perceptron_classifier <- function(X, beta) as.integer((X %*% beta) >= 0.5)

# Define the gradient for the perceptron
gradient_perceptron <- function(X.T_X_j, X.T_Y_j, beta) X.T_X_j %*% beta - X.T_Y_j 

# Define the empirical risk
empirical_risk <- function(y, y.true) mean((y-y.true)^2)

# Define the gradient descent function
gradient_descent <- function(X, Y, G, alpha=0.5, max_iter=100, tol=0.00001, beta=NULL, plot=FALSE) {
  ###
  # Function to compute the gradient descent given the gradient function G.
  # :param X : design matrix (n_observations * n_features)
  # :param Y : ground-truth labels
  # :param G : gradient function
  # :param alpha : learning rate
  # :param max_iter : maximum number of iterations; first stop condition
  # :param tol : tolerance; stop the gradient loop before max_iter is reached if the condition is met
  # :param beta : vector of parameters; provide a custom starting vector; otherwise, a random one is chosen
  # :param plot : boolean; plot the empirical risk history if TRUE 
  #
  # :return list(beta=[vector of parameters], emp_risk_hist=[empirical risk through the iterations]) 
  ###
  
  # gradient = 2/n (X.T*X*beta - X.T*Y)
  
  n <- dim(X)[1] # number of observations (data points)
  d <- dim(X)[2] # number of features
  
  if(is.null(beta))
    beta <- matrix(runif(d, min = -10, max = 10))
  
  emp_risk_hist <- c(empirical_risk(perceptron_classifier(X, beta), Y))
  
  X.T_X <- matrix(rep(0, d^2), nrow=d, ncol=d)
  X.T_Y <- matrix(rep(0, d), nrow=d, ncol=1)
  
  print("Precomputing the matrices")
  
  # The matrix operations such %*% are very cool but the transpose of the whole X overloads the memory,
  # even though doesn't generate a full copy of the object (the memory occupancy increasing is a bit less than
  # the one due to X). The for loop below, that slices the matrix column by column and then performs the operation
  # requires a very low amount of memory; the drawback is the slowness of the computation (about 9 minutes). I'm 
  # trying to vectorize the for loop to speed it up. The apply function (&Co) doesn't work, because uses quite
  # a lot of memory and is still very slow.
  for(i in 1:d) {
    X.T_X[i,] <- t(X[,i]) %*% X * 2 / n # precompute X.T * X
    X.T_Y[i,] <- t(X[,i]) %*% Y * 2 / n # pre-compute X.T * Y
  }
  
  print("Entering gradient loop...")  
  
  # After the pre-computation, the gradient loop is quite fast, even though the beta NA issue hasn't been fixed
  # yet
  
  # Gradient loop
  for(i in 1:max_iter) {

    # beta_tmp <- matrix(rep(0, d)) # tmp column vector
    # 
    # # Update the j-th component of beta [beta_j]
    # for(j in 1:d) {
    #   beta_tmp[j,] <- beta[j,] - alpha * G(X.T_X[j,], X.T_Y[j,], beta) # update rule
    # }

    # Update beta
    beta <- beta - alpha * G(X.T_X, X.T_Y, beta) # update rule

    # Compute and store the ER value of the i-th iteration
    emp_risk_hist <- cbind(emp_risk_hist, c(empirical_risk(perceptron_classifier(X, beta), Y)))
    
    print(paste("Iteration", i))
    
    # Check convergence
    tryCatch(if(abs(emp_risk_hist[i+1] - emp_risk_hist[i]) <= tol) {
      max_iter <- i
      break
    }, finally = break)
    
  }
  print(paste("Converged after", max_iter, "iterations"))

  if(plot)
    plot(seq(0,max_iter), emp_risk_hist, col="blue",
         xlab="iter", ylab="ER", main="Gradient descent")

  return(list(param=beta, emp_risk_hist=emp_risk_hist))
}

```


Below, I provide also the implementation for the logistic regression. The current implementation may suffer of insufficient RAM cause it computes the row-column product at one time.

```{r eval=FALSE, include=FALSE}
# Define the sigmoid function (logistic function)
logistic_fun <- function(X, beta) 1 / (1 + exp(-X %*% beta))


# Define the gradient for the logistic 
# To be modified with the same trick of the pre-computed matrix
# TODO: substitute the t(X) with a column-wise for loop 
gradient_logistic <- function(beta, X, XY) {
  n <- dim(X)[1] # number of observations (data points)
  return( ( ( t(X) %*% logistic_fun(X, beta) ) - XY ) / n )
}

# Define the empirical risk
empirical_risk <- function(y, y.true) mean((y-y.true)^2)


# Define the gradient descent function
gradient_descent <- function(X, Y, G, alpha=0.01, max_iter=100, tol=0.0001, beta=NULL, plot=FALSE) {
  ###
  # Function to compute the gradient descent given the gradient function G.
  # :param X : design matrix (n_observations * n_features)
  # :param Y : ground-truth labels
  # :param G : gradient function
  # :param alpha : learning rate
  # :param max_iter : maximum number of iterations; first stop condition
  # :param tol : tolerance; stop the gradient loop before max_iter is reached if the condition is met
  # :param beta: vector of parameters; provide a custom starting vector; otherwise, a random one is chosen
  # :param plot : boolean; plot the empirical risk history if TRUE 
  #
  # :return list(beta=[vector of parameters], emp_risk_hist=[empirical risk through the iterations])  
  ###
  
  # Setup
  n <- dim(X)[1] # number of observations (data points)
  d <- dim(X)[2] # number of features
  
  if(is.null(beta))
    beta <- matrix(runif(d))
  
  emp_risk_hist <- c(empirical_risk(logistic_fun(X, beta), Y))
  
  X.T_Y <- matrix(rep(0, d), nrow=d, ncol=1)
  
  print("Precomputing the matrices")
  
  # The matrix operations such %*% are very cool but the transpose of the whole X overloads the memory,
  # even though doesn't generate a full copy of the object (the memory occupancy increasing is a bit less than
  # the one due to X). The for loop below, that slices the matrix column by column and then performs the operation
  # requires a very low amount of memory; the drawback is the slowness of the computation (about 9 minutes). I'm 
  # trying to vectorize the for loop to speed it up. The apply function (&Co) doesn't work, because uses quite
  # a lot of memory and is still very slow.
  for(i in 1:d) {
    X.T_Y[i,] <- t(X[,i]) %*% Y * 2 / n # pre-compute X.T * Y
  }
  
  print("Entering gradient loop...")  
  
  # Gradient loop
  for(i in 1:max_iter) {
    
    # Update beta according to the gradient descent rule
    beta <- beta - alpha * G(beta, X, XY)
    
    emp_risk_hist <- cbind(emp_risk_hist, c(empirical_risk(logistic_fun(X, beta), Y)))
    
    # Check convergence 
    if(abs(emp_risk_hist[i+1] - emp_risk_hist[i]) <= tol) {
      max_iter <- i
      break
    }
  }
  print(paste("Converged after", max_iter, "iterations"))
  
  if(plot) 
    plot(seq(0,max_iter), emp_risk_hist, col="blue",
         xlab="iter", ylab="ER", main="Gradient descent")
  
  return(list(param=beta, emp_risk_hist=emp_risk_hist))
}
```

Map the Y string labels into \{0,1\}:

```{r}
encode <- function(Y) {
  unique_labs <- unique(Y)
  Y_encoded <- rep(0, length(Y))
  for(i in 1:length(Y)) {
    Y_encoded[i] <- as.integer(Y[i] == unique_labs[2])
  }
  return(matrix(Y_encoded))
}
```

Take a small random subset of the train set:

```{r eval=FALSE, include=FALSE}
n <- length(y_tr)
idxs <- sample(seq(1, n), size = n, replace = FALSE)
```

Encode the labels and compute the parameters of the trained model:

```{r}
y_tr_encoded <- encode(y_tr)# encode(y_tr[idxs])

result <- gradient_descent(X_tr, y_tr_encoded, gradient_perceptron, alpha=0.3, max_iter=200, plot=TRUE)
```

Take a small random subset of the test set:

```{r}
n <- length(y_te)
idxs <- sample(seq(1, n), size = 1000, replace = FALSE)
```

Encode the labels of the ground-truth vector and compute the predictions:

```{r}
t_te_encoded <- encode(y_te[idxs])

pred <- as.integer(c(perceptron_classifier(X_te[idxs,], beta)) >= 0.5)
```

Rapidly compute the number of right predictions:

```{r}
perf <- sum(pred == t_te_encoded) / 1000
```

Check the number of right predictions:

```{r}
perf
```

