---
title: "HOMEWORK 01"
author: "G01: Betti, D'Arrigo, Masci, Mata Naranjo"
output: html_document
header-includes:
- \usepackage{bbold}
- \usepackage{hyperref}
- \usepackage{mdframed, xcolor}
- \usepackage{graphicx}
- \usepackage{amsmath}
- \mdfsetup{frametitlealignment=\center}
- \usepackage{multirow}
- \usepackage{bbold}
- \usepackage{amsfonts}
- \usepackage{mathtools}
- \definecolor{shadecolor}{rgb}{0.89,0.8,1}
- \newcommand{\I}{\mathbb{I}}
- \newcommand{\Prob}{\mathbb{P}}
- \newcommand{\Exp}{\mathbb{E}}
- \newcommand{\Var}{\mathbb{V}\mathrm{ar}}
- \newcommand{\Cov}{\mathbb{C}\mathrm{ov}}
- \newcommand{\blue}{\textcolor{blue}}
- \newcommand{\darkgreen}{\textcolor[rgb]{0,.5,0}}
- \newcommand{\gray}{\textcolor[rgb]{.3,.3,.3}}
- \newcommand{\blueA}{\textcolor[rgb]{0,.1,.4}}
- \newcommand{\blueB}{\textcolor[rgb]{0,.3,.6}}
- \newcommand{\blueC}{\textcolor[rgb]{0,.5,.8}}
- \newcommand{\evidenzia}{\textcolor[rgb]{0,0,0}}
- \newcommand{\nero}{\textcolor[rgb]{0,0,0}}
- \newcommand{\darkyel}{\textcolor[rgb]{.4,.4,0}}
- \newcommand{\darkred}{\textcolor[rgb]{.6,0,0}}
- \newcommand{\blueDek}{\textcolor[rgb]{0.6000000, 0.7490196, 0.9019608}}
- \newcommand{\purpLarry}{\textcolor[rgb]{0.6901961, 0.2431373, 0.4784314}}
- \newcommand{\lightgray}{\textcolor[rgb]{.8,.8,.8}}
- \newcommand{\bfun}{\left\{\begin{array}{ll}}
- \newcommand{\efun}{\end{array}\right.}
---

```{r library, echo=FALSE, message=FALSE, warning=FALSE}
require(viridis) 
library(DT)
```

<br><br>

# Part 1 - The Bayes Classifier

TODO: Leo

<br>

### 1. Define the Bayes classifier/strategy and briefly explain its role/importance in classification/prediction.

<br>

\textbf{Include here some theory...}

<br>

### 2. Find (with pen and paper) the Bayes classification rule $h_{opt}(x)$.

<br>

It can be proven that under the following conditions:

\begin{equation}
Y \in \{0, 1\}
\end{equation}

\begin{equation}
X \in \mathbb{R}
\end{equation}

the optimal classifier is:

\begin{equation}
h_{opt}(x) = \I(f_{opt}(x) - 1/2 \geq 0)
\end{equation}

In order to estimate the optimal Bayes classification rule we have to derive $f_{opt}(x)$ by using the Bayes Rule:

\begin{equation}
\begin{split}
f_{opt}(x) 
&= \Exp(Y=1|X=x) = \Prob(Y=1|X=x) = \\
&= \frac{\Prob(X|Y=1)\Prob(Y=1)}{\Prob(X|Y=1)\Prob(Y=1)+\Prob(X|Y=0)\Prob(Y=0)} = \\
&\stackrel{i}{=} \frac{\Prob(X|Y=1)}{\Prob(X|Y=1)+\Prob(X|Y=0)} = \\
&\stackrel{ii}{=} \frac{\I_{[-1, 3]}(x)}{\I_{[-1, 3]}(x)+\I_{[-3, 1]}(x)}
\end{split}
\end{equation}

where we have used that (i) $P(Y=0) = P(Y=1)$ and (ii) $(X|Y=0) \sim Unif(-3, 1)$ and $(X|Y=1) \sim Unif(-1, 3)$. So finally we have that:

\begin{equation}
h_{opt}(x) = \I\bigg(\frac{\I_{[-1, 3]}(x)}{\I_{[-1, 3]}(x)+\I_{[-3, 1]}(x)} - \frac{1}{2} \geq 0\bigg)
\end{equation}


<br>

### 3. Simulate n = 250 data from the joint data model p(y, x) = p(x | y) · p(y) described above, and then:

#### a. Plot the data together with the regression function that defines $h_{opt}(x)$

```{r Plot Regression, fig.width=18, fig.height=10}
# Generate Random Data
n <- 250
y <- sample(c(0,1), size=n, replace=TRUE, prob=c(.5,.5))
x_given_y <- c()
for (yi in y){
  if (yi==0){
    x <- runif(1, -3, 1)
  } else {
    x <- runif(1, -1, 3)
  }
  x_given_y <- c(x_given_y, x)
}

# Optimal Bayes Classifier
h.opt <- function(x) {
    
  f.opt = (ifelse((x<=3&x>=-1), 1, 0))/(ifelse((x<=3&x>=-1), 1, 0) + ifelse((x<=1&x>=-3), 1, 0))
  res = ifelse(f.opt-.5>=0, 1, 0)
  return(res)
}

# Plot + Regression Function
mycol = viridis(7, alpha = .7)
plot(x = x_given_y, y = y, bg= ifelse(y == 1, mycol[1], mycol[7]),
     pch=21, xlab="X", ylab = "Y", main="Simulated Data Points + Optimal Bayes Classifier", cex=3, 
     cex.lab=1.5, cex.axis=1.5, cex.main=2)
abline(h = 0.5, lty = 3, col = gray(.8))
abline(v = 0, lty = 3, col = gray(.8))
rug(x_given_y)
curve(h.opt(x), from=-3, to=3, add = TRUE, col="gray", lwd=3)

```

<br>

#### b. Evaluate the performance of the Bayes Classifiers on these simple (only 1 feature!) data

The performance evaluation will be done using the binary loss function (which is the loss function used to derived the $h_{opt}()x$ function used to construct this Bayes Classifier):

```{r Loss Evaluator}
# Loss Function
loss <- function(y.actual, y.pred) {
  
  res = sum(y.actual != y.pred)
  return(res)
}

print(paste("Bayes Classifier Loss:", loss(y, h.opt(x_given_y))))
print(paste("Relative Bayes Classifier Loss:", loss(y, h.opt(x_given_y))/length(y)))

```
<br>

#### c. Apply any other classifier of your choice to these data and comparatively comment its performance

<br>

In this section we will choose another Classifier to compare its performance against that of the Bayes Classifier. The most obvious option at this point seems to be the Logistic Regression, so we will give this Classifier a try:

```{r Logistic Regression, fig.width=18, fig.height=10}

logistic_regression <- function(x, y) {
  
  data = data.frame(y, x)
  fit_logit <- glm(y ~ ., data = data, family = "binomial")
  params = coef(fit_logit)
  return(params)
}

logit_func <- function(x, parameters=params) 1/(1 + exp(-(parameters[1]+parameters[2]*x)))

params = logistic_regression(x=x_given_y, y=y)

# Plot + Regression Function
plot(x = x_given_y, y = y, bg= ifelse(y == 1, mycol[1], mycol[7]),
     pch=21, xlab="X", ylab = "Y", main="Simulated Data Points + Optimal Bayes Classifier", cex=3, 
     cex.lab=1.5, cex.axis=1.5, cex.main=2)
abline(h = 0.5, lty = 3, col = gray(.8))
abline(v = 0, lty = 3, col = gray(.8))
rug(x_given_y)
curve(h.opt(x), from=-3, to=3, add = TRUE, col="gray", lwd=3)
curve(logit_func(x, parameters=params), from=-3, to=3, add = TRUE, col="orchid", lwd=3)
curve(round(logit_func(x, parameters=params)), from=-3, to=3, add = TRUE, col="black", lwd=3)
legend(x=-3,, y=1, 
       legend=c("h Opt Bayes", "Logistic Regression", "h Opt Logistic Regression"), 
       #legend=c("1", "2", "3"), 
       col=c("gray", "orchid", "black"), cex=1.5, lty=1, lwd=3)

print(paste("Logistic Regression Classifier Loss:", loss(y, round(logit_func(x=x_given_y, parameters=params)))))
print(paste("Relative Logistic Regression Classifier Loss:", loss(y, round(logit_func(x=x_given_y, parameters=params)))/length(y)))

```

<br>

#### d. Who’s the best after 1000 simulations?

<br>

In order to compare both classifiers we will perform 1000 simulations and estimate the loss for each of the random samples. 

```{r Simulation, fig.width=18, fig.height=10}

M = 1000
res_bayes = rep(NA, M)
res_log = rep(NA, M)

for(sim in 1:M){
  
  #if(sim%%10 == 0){
  #  print(sim)
  #}
  # Simulate Random Data
  n <- 250
  y <- sample(c(0,1), size=n, replace=TRUE, prob=c(.5,.5))
  x_given_y <- c()
  for (yi in y){
    if (yi==0){
      x <- runif(1, -3, 1)
    } else {
      x <- runif(1, -1, 3)
    }
    x_given_y <- c(x_given_y, x)
  }
  
  # Make predictions with Logistic Regression
  params = logistic_regression(x=x_given_y, y=y)
  res_log[sim] = loss(y, round(logit_func(x=x_given_y, parameters=params)))
  
  # Make predictions with Bayes
  res_bayes[sim] = loss(y, h.opt(x_given_y))
  
}

print(paste("Average Bayes Classifier Loss:", mean(res_bayes)))
print(paste("Average Logistic Regression Classifier Loss:", mean(res_log)))

par(mfrow = c(1:2))
hist(res_log, prob=TRUE, main="Distribution of \n Logistic Regression Loss", xlab="Loss", col=mycol[4], ylim = c(0, 0.07), breaks=20, cex.lab=1.5, cex.axis=1.5, cex.main=2)
hist(res_bayes, prob=TRUE, main="Distribution of \n Bayes Loss", xlab="Loss", col=mycol[6], ylim = c(0, 0.07), breaks=20, cex.lab=1.5, cex.axis=1.5, cex.main=2)

```

The previous plots show the distribution of number of miss-classified data points over 1000 simulations. We see how in both cases the distribution has a Gaussian-like shape. However, repeating this process multiple times we have observed  that the Bayes Classifier gives a more stable loss distribution, while the Logistic Regression leads to some more skewed and with higher variance distributions. 

The Logisitic Regression Classifier outperforms the Bayes classifier, however, not by much. The reason for which Logistic Regression is able to classify better is due to the fact that it classifies data points dynamically in the interval $x \in [-1, 1]$, while the Bayes Classifier will always assign these points to the class $Y=1$. Theoretically, the Bayes approach should be the ideal classifier (since the data points are equally distributed among $Y=1$ and $Y=0$ in this interval), however since our data generating process only generates 250 points, the real distribution in the $[-1, 1]$ interval is not exactly equally distribute.



