---
title: "Interpretable Machine Learning"
author: "Study of Bibliography"
date: "24.03.2021"
output: pdf_document
urlcolor: magenta
linkcolor: cyan
geometry: margin=1.25cm
fontsize: 12pt
header-includes:
- \usepackage{bbold}
- \usepackage{bbm}
- \usepackage{mdframed, xcolor}
- \usepackage{graphicx}
- \usepackage{amsmath}
- \mdfsetup{frametitlealignment=\center}
- \usepackage{multirow}
- \definecolor{shadecolor}{rgb}{0.89,0.8,1}
- \newcommand{\Prob}{\mathbb{P}}
- \newcommand{\Exp}{\mathbb{E}}
- \newcommand{\R}{\mathbb{R}}
- \newcommand{\Var}{\mathbb{V}\mathrm{ar}}
- \newcommand{\Cov}{\mathbb{C}\mathrm{ov}}
- \newcommand{\I}{\mathbbm{1}}
- \newcommand{\blue}{\textcolor{blue}}
- \newcommand{\darkgreen}{\textcolor[rgb]{0,.5,0}}
- \newcommand{\gray}{\textcolor[rgb]{.3,.3,.3}}
- \newcommand{\blueA}{\textcolor[rgb]{0,.1,.4}}
- \newcommand{\blueB}{\textcolor[rgb]{0,.3,.6}}
- \newcommand{\blueC}{\textcolor[rgb]{0,.5,.8}}
- \newcommand{\evidenzia}{\textcolor[rgb]{0,0,0}}
- \newcommand{\nero}{\textcolor[rgb]{0,0,0}}
- \newcommand{\darkyel}{\textcolor[rgb]{.4,.4,0}}
- \newcommand{\darkred}{\textcolor[rgb]{.6,0,0}}
- \newcommand{\blueDek}{\textcolor[rgb]{0.6000000, 0.7490196, 0.9019608}}
- \newcommand{\purpLarry}{\textcolor[rgb]{0.6901961, 0.2431373, 0.4784314}}
- \newcommand{\lightgray}{\textcolor[rgb]{.8,.8,.8}}
- \newcommand{\bfun}{\left\{\begin{array}{ll}}
- \newcommand{\efun}{\end{array}\right.}
---


A Unified Approach to Interpreting Model Predictions
----------
The main idea of this paper is to present a framework for interpreting predictions using SHAP (Shapley Additive Explanations). This framework assigns each feature an importance value for a particular prediction. It's main components include (GitHub repo is https://github.com/slundberg/shap):

\begin{itemize}
  \item Identification of a new class of additive feature importance measures
  \item Theoretical results showing there is a unique solution in this class with a set of desirable properties
\end{itemize}

### 1. Additive Feature Attribution Methods

The best explanation of a simple model is the model itself; it perfectly represents itself and is easy to understand. For complex models, such as ensemble methods or deep networks, we cannot use the original model as its own best explanation because it is not easy to understand. The idea to explain this more complex model is to use a simpler explanation model. From now on we will use $f$ to be the original prediction model that needs to be explained, and $g$ the explanation model.

### 2. Simple Properties Uniquely Determine Additive Feature Attributions


### 3. SHAP (SHapley Additive exPlanation) Values


### 4. Computational and User Study Experiments





----------


Causal Interpretability for Machine Learning - Problems, Methods and Evaluation
----------

\textit{This seems like a "easy" to understand paper}

With the surge of machine learning in critical areas such as healthcare, law-making and autonomous cars, decisions that had been previously made by humans are now made automatically using these algorithms. In order to ensure the reliability of such decisions, humans need to understand how these decisions are made. However, machine learning models are usually inherently black-boxes and do not provide explanations for how and why they make such decisions. This has become especially problematic when recent work shows that the decisions made by machine learning models are sometimes biased and enforce inequality. Understanding decisions of machine learning models and the process leading to decision making can help us understand the rules the models use to make their decisions and therefore, prevent potential unexpected situations from happening. 

In this work, we focus on causal interpretable models that can explain their decisions through what decisions would have been made if they had been under alternative situations (e.g., being trained with different inputs, model components or hyperparameters). “What would have happened to this decision of a classifier had we had a different input to it?”, or “Was it feature X that caused decision Y ?”. 

### 1. An Overview of Interpretability

We categorize traditional models into two main categories:

\begin{itemize}
  \item \textbf{Inherently interpretable models:} Models that generate explanations in the process of decision making or while being trained (e.g. \textit{Decision Trees}, \textit{Rule-Based Models}, \textit{Linear Regression}, \textit{Attention Networks},  .
  \item \textbf{Post-hoc interpretability:} Generating explanations for an already existing model using an auxiliary model. Example-based interpretablity also falls into this category. In example-based interpretablity, we are looking for examples from the dataset which explain the model’s behavior the best (These methods map an abstract concept used in a trained machine learning model into a domain that is understandable by humans such as a block of pixels or a sequence of words). Examples could be:
  
\end{itemize}

### 3. SHAP (SHapley Additive exPlanation) Values


### 4. Computational and User Study Experiments






----------


Model-Agnostic Counterfactual Explanations for Consequential Decisions
----------

\subsubsection{Summary}

This approach basically tries to retrieve all of the features that would have changed the outcome of a model (this paper is mainly focused on the binary classification problem but it is mentioned in the conclusion that we should be able to apply this also to regression models) and pick those that minimize the distance between the original features of the model $\boldsymbol{x}$ and all the features that would have produced a different outcome/result $\hat{\boldsymbol{x}}$. The main idea is clear, however it is still not clear to me how we can obtain the results, i.e. how the optimization problem works (it relies on SMT which needs to be reviewed).

\subsubsection{Details}

In the context of consequential decision making, it is widely agreed that a good explanation should provide answers to the following two questions:

\begin{itemize}
  \item "Why does the model output a certain prediction for a given individual?"
  \item "What features describing the individual would need to change to achieve the desired output?"
\end{itemize}

This paper will focus on the second of these two questions, and specifically we will concentrate ourselves on finding the \textit{nearest counterfactual explanation}, identifying the set of features resulting in the desired prediction while remaining at minimum distance from the original set of features describing the individual.

There are already several approaches out there that are able to tackle this problem, but they come with a wide range of restrictions on the models itself (model need to be convex or differentiable). Most approaches also rely on homogeneous data, but we should also consider cases in which we have heterogeneous data, some of which cannot be changed (sex, age, race, etc.).

Github account where code can be found \textit{https://github.com/amirhk/}.

Lets assume we have a predictive model that maps a an input features vector $\boldsymbol{x}$ into the {0, 1} space (so we are using features to make a binary classification). In other words $f : X \rightarrow \{0, 1\}$. We can then define the \textit{set of counterfactuals explanations} as $CF_f(\hat{\boldsymbol{x}}) = \{\boldsymbol{x} \in X \ | \ f(\boldsymbol{x})\neq(\hat{\boldsymbol{x}})\}$. In words, $CF_f(\hat{\boldsymbol{x}})$ contains all the inputs x for which the model $f$ returns a prediction different from $f(\hat{\boldsymbol{x}})$ (so basically we are fixing $\hat{\boldsymbol{x}}$ and also $f$, and we look for all of the input feature vectors that live in $X$ such that the predictions are different).

For a predictive model $f : X \rightarrow \{0,1\}$, with $\boldsymbol{x}$ (input) and $y$ output, the characteristic formula $\phi_f$ verifies that $\phi_f(\boldsymbol{x}, y)$ is valid if and only if $f(\boldsymbol{x})=y$. So if for a fixed $\hat{\boldsymbol{x}}$ we have $f(\hat{\boldsymbol{x}})=\hat{y}$ then:

\begin{align}
\phi_{CF_f(\hat{\boldsymbol{x}})}(\boldsymbol{x}) = \phi_f(\boldsymbol{x}, 1-\hat{y})
\end{align}

It is thus clear from the definition that an input $\boldsymbol{x}$ satisfies $\phi_{CF_f(\hat{\boldsymbol{x}})}$ if and only if $\boldsymbol{x} \in CF_f(\hat{\boldsymbol{x}})$.

Lets look at two examples for the ease of illustration (refer to figures 2 in the original paper):

First example is about the decision tree that takes inputs in the space $(x_1, x_2, x_3) \in \{0, 1\}^2 \times \R$ returning a binary value. The clause corresponding to the leftmost leaf in the tree is given by $(x_1 = 1  \wedge x_3>0 \wedge y=0)$. The characteristic formula $\phi_f(\boldsymbol{x},y)$ is given by combining all such clauses.

So based on the counterfactual space $CF_f(\hat{\boldsymbol{x}})$, we would like to produce counterfactual explanations for the output of a model $f$ on a given input $\hat{\boldsymbol{x}}$ by trying to find a neartes counterfactual, defined as:

\begin{align}
\hat{\boldsymbol{x}}^* \in \operatorname*{argmin}_{\boldsymbol{x} \in CF_f(\hat{\boldsymbol{x}})} d(\boldsymbol{x}, \hat{\boldsymbol{x}})
\end{align}

(look for all the $\hat{\boldsymbol{x}}^*$ such that the distance between input vectors $\boldsymbol{x}$ that belong to the counterfactuals minimizes the distance to $\hat{\boldsymbol{x}}$).

The question know is how to solve the previous equation? The answer is trying to leverage the representation of $CF_f(\hat{\boldsymbol{x}})$ in terms of a logic formula.







----------

Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead
----------

Rather than trying to create models that are inherently interpretable, there has been a recent explosion of work on ‘explainable ML’, where a second (post hoc) model is created to explain the first black box model. This is problematic. Explanations are often not reliable, and can be misleading, as we discuss below. If we instead use models that are inherently interpretable, they provide their own explanations, which are faithful to what the model actually computes. As the term is presently used in its most common form, an explanation is a separate model that is supposed to replicate most of the behaviour of a black box (for example, ‘the black box says that people who have been delinquent on current credit are more likely to default on a new loan’).

\textbf{It is a myth that there is necessarily a trade-off between accuracy and interpretability.} When considering problems that have structured data with meaningful features, there is often no significant difference in performance between more complex classifiers (deep neural networks, boosted decision trees, random forests) and much simpler classifiers (logistic regression, decision lists) after preprocessing.

\textbf{Explainable ML methods provide explanations that are not faithful to what the original model computes. Explanations must be wrong.} They cannot have perfect fidelity with respect to the original model. If the explanation was completely faithful to what the original model computes, the explanation would equal the original model, and one would not need the original model in the first place, only the explanation. (In other words, this is a case where the original model would be interpretable.) This leads to the danger that any explanation method for a black box model can be an inaccurate representation of the original model in parts of the feature space. 

Explanations often do not make sense or do not provide enough detail to understand what the black box is doing. For example when using saliency maps in computer vision (where wide spread), the explanatory model is basically telling us where the model is focusing but it is not telling us what the model is exactly doing with that information (so we still have a partial explanation). Another misleading trend nowadays is the explanation of models only when the label is correct, not when the model is predicting incorrectly (it can be of the same use to understand why a model is performing badly than to know why a model is performing correctly).

Unfortunately, there are many cases where black boxes with explanations are preferred over interpretable models, even for high-stakes decisions. However, for most applications, I am hopeful that there are ways around some of these problems, whether they are computational problems or problems with training of researchers and availability of code. The first problem, however, is currently a major obstacle that I see no way of avoiding other than through policy, as discussed in the next section. 

\textbf{Corporations can make profits from the intellectual property afforded to a black box.} Companies that charge for individual predictions could find their profits obliterated if an interpretable model were used instead. An example of where this can go wrong is given by Zech and colleagues32, who noticed that their neural network was picking up on the word ‘portable’ within an X-ray image, representing the type of X-ray equipment rather than the medical content of the image. If they had used an interpretable model, or even an explainable model, this issue would never have gone unnoticed. 

\subsubsection{Algorithmic challenges in interpretable Machine Learning}

Because interpretability is domain-specific, a large toolbox of possible techniques can come in handy. Below we expand on three of the challenges for interpretable ML that appear often.

\textbf{Challenge 1: Constructing optimal logical models.} A logical model consists of statements involving ‘or’, ‘and’, ‘if–then’ and so on. We would like models that look like they are created by hand, but they need to be accurate, full-blown ML models. To this end, let us consider the following optimization problem, which asks us to find a model that minimizes a combination of the fraction of misclassified training points and the size of the model. Being i the traning observation that goes from 1 to n, and $F$ the family of logical models, the optimization problem is as follows:

\begin{align}
\operatorname*{min}_{f \in F} \bigg( \frac{1}{n} \sum_{i=1}^n \I(\textnormal{i is missclassified by f}) + \lambda \times \textnormal{size}(f) \bigg)
\end{align}

where the size of $f$ can be measured by the number of conditions or leaves in a decision tree, and $\lambda$ the classification error one would sacrifice to reduce by having one term fewer. The issue of course is that the optimization of such an equation is very complex:

\begin{itemize}
\item Interpretable models sometimes entail hard computational problems
\item These computational problems can be solved by leveraging a combination of theoretical and systems-level techniques
\end{itemize}

\textbf{Challenge 2: Construct optimal sparse scoring systems.} 

Not sure what additional information is being provided to us in this section...

\textbf{Challenge 3: Define interpretability for specific domains and create methods accordingly, including computer vision.} Because interpretability needs to be defined in a domain-specific way, some of the most important technical challenges for the future are tied to specific important domains. Let us start with computer vision, for the classification of images. There is a vast and growing body of research on post hoc explainability of deep neural networks, but not as much work in designing interpretable neural networks. For computer vision in particular, there is not a clear definition of interpretability, and the sparsity-related models discussed above do not apply—sparsity in pixel space does not make sense. During training, the prototype layer finds parts of training images that act as prototypes for each class. For example, for bird classification, the prototype layer might pick out a prototypical head of a blue jay, prototypical feathers of a blue jay and so on. The network also learns a similarity metric between parts of images. Thus, during testing, when a new test image needs to be evaluated, the network finds parts of the test image that are similar to the prototypes it learned during training.





