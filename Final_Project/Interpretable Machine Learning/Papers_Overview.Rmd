---
title: "Interpretable Machine Learning"
author: "Study of Bibliography"
date: "24.03.2021"
output: pdf_document
urlcolor: magenta
linkcolor: cyan
geometry: margin=1.25cm
fontsize: 12pt
header-includes:
- \usepackage{bbold}
- \usepackage{mdframed, xcolor}
- \usepackage{graphicx}
- \mdfsetup{frametitlealignment=\center}
- \usepackage{multirow}
- \definecolor{shadecolor}{rgb}{0.89,0.8,1}
- \newcommand{\Prob}{\mathbb{P}}
- \newcommand{\Exp}{\mathbb{E}}
- \newcommand{\Var}{\mathbb{V}\mathrm{ar}}
- \newcommand{\Cov}{\mathbb{C}\mathrm{ov}}
- \newcommand{\blue}{\textcolor{blue}}
- \newcommand{\darkgreen}{\textcolor[rgb]{0,.5,0}}
- \newcommand{\gray}{\textcolor[rgb]{.3,.3,.3}}
- \newcommand{\blueA}{\textcolor[rgb]{0,.1,.4}}
- \newcommand{\blueB}{\textcolor[rgb]{0,.3,.6}}
- \newcommand{\blueC}{\textcolor[rgb]{0,.5,.8}}
- \newcommand{\evidenzia}{\textcolor[rgb]{0,0,0}}
- \newcommand{\nero}{\textcolor[rgb]{0,0,0}}
- \newcommand{\darkyel}{\textcolor[rgb]{.4,.4,0}}
- \newcommand{\darkred}{\textcolor[rgb]{.6,0,0}}
- \newcommand{\blueDek}{\textcolor[rgb]{0.6000000, 0.7490196, 0.9019608}}
- \newcommand{\purpLarry}{\textcolor[rgb]{0.6901961, 0.2431373, 0.4784314}}
- \newcommand{\lightgray}{\textcolor[rgb]{.8,.8,.8}}
- \newcommand{\bfun}{\left\{\begin{array}{ll}}
- \newcommand{\efun}{\end{array}\right.}
---


A Unified Approach to Interpreting Model Predictions
----------
The main idea of this paper is to present a framework for interpreting predictions using SHAP (Shapley Additive Explanations). This framework assigns each feature an importance value for a particular prediction. It's main components include (GitHub repo is https://github.com/slundberg/shap):

\begin{itemize}
  \item Identification of a new class of additive feature importance measures
  \item Theoretical results showing there is a unique solution in this class with a set of desirable properties
\end{itemize}

### 1. Additive Feature Attribution Methods

The best explanation of a simple model is the model itself; it perfectly represents itself and is easy to understand. For complex models, such as ensemble methods or deep networks, we cannot use the original model as its own best explanation because it is not easy to understand. The idea to explain this more complex model is to use a simpler explanation model. From now on we will use $f$ to be the original prediction model that needs to be explained, and $g$ the explanation model.

### 2. Simple Properties Uniquely Determine Additive Feature Attributions


### 3. SHAP (SHapley Additive exPlanation) Values


### 4. Computational and User Study Experiments





----------


Causal Interpretability for Machine Learning - Problems, Methods and Evaluation
----------

\textit{This seems like a "easy" to understand paper}

With the surge of machine learning in critical areas such as healthcare, law-making and autonomous cars, decisions that had been previously made by humans are now made automatically using these algorithms. In order to ensure the reliability of such decisions, humans need to understand how these decisions are made. However, machine learning models are usually inherently black-boxes and do not provide explanations for how and why they make such decisions. This has become especially problematic when recent work shows that the decisions made by machine learning models are sometimes biased and enforce inequality. Understanding decisions of machine learning models and the process leading to decision making can help us understand the rules the models use to make their decisions and therefore, prevent potential unexpected situations from happening. 

In this work, we focus on causal interpretable models that can explain their decisions through what decisions would have been made if they had been under alternative situations (e.g., being trained with different inputs, model components or hyperparameters). “What would have happened to this decision of a classifier had we had a different input to it?”, or “Was it feature X that caused decision Y ?”. 

### 1. An Overview of Interpretability

We categorize traditional models into two main categories:

\begin{itemize}
  \item \textbf{Inherently interpretable models:} Models that generate explanations in the process of decision making or while being trained (e.g. \textit{Decision Trees}, \textit{Rule-Based Models}, \textit{Linear Regression}, \textit{Attention Networks},  .
  \item \textbf{Post-hoc interpretability:} Generating explanations for an already existing model using an auxiliary model. Example-based interpretablity also falls into this category. In example-based interpretablity, we are looking for examples from the dataset which explain the model’s behavior the best (These methods map an abstract concept used in a trained machine learning model into a domain that is understandable by humans such as a block of pixels or a sequence of words). Examples could be:
  
\end{itemize}

### 3. SHAP (SHapley Additive exPlanation) Values


### 4. Computational and User Study Experiments






----------


Model-Agnostic Counterfactual Explanations for Consequential Decisions
----------

\tetxit{This paper seems to go through the results of the github account mentioned at the beginning}







----------

IML paper(s) you like (at this point!)
----------

*List here the papers on interpretable/explainable ML and related fields that you'd like to integrate in your work. Also please spend few words to tell me why you like them.*

-------

Data source(s) 
----------
*Differently from previous years, the [Science Journal App](https://sciencejournal.withgoogle.com/experiments/) (now called [Arduino Science Journal](https://apps.apple.com/us/app/arduino-science-journal/id1518014927), see also [here](https://science-journal.arduino.cc/)) is not mandatory for this project.*
*Hence, please describe the software/hardware/tools/platforms/apps/devices/etc you are planning to use in order to get the data you need.*

----------

Data collection
----------
*Explain how you are plannig to actually collect the data.*
*How many data-points do you (realistically) hope to collect?*
*Do you foresee any difficulty in the data collection process?*
*Approximatively, how "heavy" do you think your dataset will be in the end?*

----------

Model \& Methods
----------
*To the best of your current knowledge, what type of statistical/data analysis tools do you feel will be relevant?*
*Explain briefly why.*

*If there's some cutting edge methods you may like to implement, cite it here adding the relevant paper(s)/book(s) in the last Section of this document.*

*In particular, how the IML analysis described in the papers you selected fits into your project*

----------


Software/Hardware Toolkit
----------
*Tell me something about the software, programming language(s), package(s), module(s), framework(s) you're planning to use to handle/model/analyse your data and why.*
*Are you also planning to use or develop dedicated hardware for this task?*
*If you feel your home-computer/laptop may not be enough to handle everything, explain to me why and what kind of resources you may instead need.*

----------

Project Timeline
----------
*Assuming I like and approve your proposal, detail how you are planning to organize your work in terms of sub-tasks completion.
If you know what a [Gantt diagram](https://en.wikipedia.org/wiki/Gantt_chart) is, [use it](https://plot.ly/r/gantt/)!*

----------


References
----------
*List here all the reference cited above. If you know what it is, it will be very much appreciated if you use and then upload on Moodle a [.bib file](https://rmarkdown.rstudio.com/authoring_bibliographies_and_citations.html#bibliographies).*
*As an example:*

- Lehmann, E. L. and Casella, G. (1998). *Theory of Point Estimation.* Springer-Verlag.
- Hastie, T., Tibshirani, R. and Friedman, J. H. (2001). *The Elements of Statistical Learning: Data Mining, Inference, and Prediction.* Springer-Verlag.
- Shen, X. and Wasserman, L. (2001). *Rates of convergence of posterior distributions.* The Annals of Statistics, **29**, pp. 687--714.

----------
