---
title: "HOMEWORK 01"
output: html_document
---

```{r library, echo=FALSE, message=FALSE, warning=FALSE}

require(viridis) 
library(DT)

```

### Part 1 - Linear Algebra is good...

#### Question 3.a.

We will try to explain this parallelism by first reviewing the logic used in the Transformed Feature Space.

As we have seen in class, having a finite number of feature variables (for the sake of simplicity lets make this 1 and denote it by $x$), we could use them to predict another target variable (1 dimensional, $y$) through a linear combination as follows:

\begin{align}
y = \beta_0 + \beta_1x
\end{align}

Such models assumption might not be sufficient to capture all the information in $y$ through this simple transformation (when projecting $y$ on the 1-dimensional space of $x$ we might loose too much information). For this reason, we might want to think of a more complex set of transformations we can apply on our feature $x$ in order to capture more details about the target variable. This can be accomplished through a function/transformation $\phi(x)$ such that:

\begin{align}
y = \sum_{j=1}^n \beta_j + \phi_j(x)
\end{align}

This is basically the same principle used in the series expansion in which we try to expand a function $m(x)$ (possibly complex) by adding up less complex and therefore easier to handle functions together:

\begin{align}
m(x) = \sum_{j=1}^n \beta_j + \phi_j(x)
\end{align}

It is important to highlight that one of the basic reasons for which these two principles work (in the vector and function space) is because in both case it is possible to exploit the dot product property between the elements living in the respective spaces.

Finally, we would also like to highlight that very nice properties pop up in both scenarios if the transformations ($\phi_j(x)$) form an orthonormal set of functions/vectors:

- Adding additional components to the sum would not change the previous $\beta_j$ values.
- $\beta_j$ can be estimated using the Generalized Fourier Expansion, which in the limit tends to an interpolation function of y or $m(x)$.

#### Question 3.b/c.

We know that a function $m(\cdot)$ can be approximated by less complex functions through a linear combination of Fourier coefficients. In order to find the best approximation we must define a system to work with. If we are in a Hilbert space $H$ with an orthonormal basis $\phi_j(x)$, the best approximant is the partial sums $m=\sum_{j=1}^J<m(x),\phi_j(x)>$, where $m(x)$ is just a linear, $J$ dimensional subspace of $H$. 

However, to provide a better approximation error, it may be convenient to use a non-linear approximation of the form $m_J^*(x)$ to the Doppler function, which requires less Fourier coefficients (reducing the $||m||^2$).
For a fixed $J$, $m_J^*(x)$ has the same cardinality (and therefore complexity) as $m_J(x)$, but in the non linear case the approximants $\{\beta_j\}^J_j$ come from different linear sub-spaces.
We know that, since the *Fourier* coefficients are the optimal choice to minimize $\Delta_J=||m-\sum_{j=1}^Jc_j\phi_j||$, the best choice of $c_j$ is $\beta$. Consequently, the best choice of $\Delta_J$ is the set of the indices $J$ with the biggest coefficients in absolute value of $\beta$. 

It is important to underline that this applies only to the extremely simple case of a Hilbert space with an orthonormal base, therefore we will consider a cosine basis.

### Defining Cosine-basis and Doppler function scaled in [0,1]

```{r setup}

cos.basis = function(x, j) 1*(j == 0) + sqrt(2)*cos(pi*j*x)*(j > 0)
doppler.fun <-  function(x) sqrt(x*(1 - x))*sin( (2.1*pi)/(x + 0.05))

```

### Fourier coefficients of the Doppler under cosine-basis

```{r fourier coefficients}

j.max   <- 200
f.coeff <- rep(NA, j.max+1)

for (idx in 0:j.max){
  foo = tryCatch(
    integrate(function(x, j) doppler.fun(x) * cos.basis(x,j), lower = 0, upper = 1, j = idx)$value,
    error = function(e) NA
  )
  f.coeff[idx + 1] = foo
}

```

### Computing approximation
In the linear case we pick the the first m (j.max) Fourier transforms, while in the non-linear case we pick m indices corresponding to the greater Fourier transforms.

```{r funcs}


# sorting indexes of Fourier transform
idx_sorted <- sort(abs(f.coeff), decreasing = T, index.return = T)$ix

# Linear
J_linear <- function(x, f.coeff = f.coeff, j.max){
  out = rep(0, length(x))
  for(idx in 0:j.max){
    if(!is.na(f.coeff[idx + 1])) out = out + f.coeff[idx + 1] * cos.basis(x, j = idx)
  }
  return(out)
}

# Non linear
J_nonlinear <- function(x, f.coeff = f.coeff, j.max){
  out = rep(0, length(x))
  for(idx in head(idx_sorted,j.max)){
    if(!is.na(f.coeff[idx])) out = out + f.coeff[idx] * cos.basis(x, j = idx-1)
  }
  return(out)
}

```

### Plots
We visualize some n-terms approximations

```{r pressure, fig.width=20, fig.height=10}

par(mfrow = c(2,3))
j.seq = c(5, 10, 25, 50, 100, 150)
mycol = viridis(length(j.seq), alpha = .7)


# Linear
error_linear = c()

f_linear <- function(f.coeff){
  for (idx in 1:length(j.seq)){
     
    curve(doppler.fun(x), from = 0, to = 1, 
          main = paste(j.seq[idx], "-term approximation", sep = ""), xlab = "", 
          ylab = expression(m[J](x)), n = 1001, col = gray(.8), lwd = 3)
    
    curve(J_linear(x, f.coeff = f.coeff , j.seq[idx]),
          n = 1001, col = mycol[idx], lwd = 5,add = TRUE)

    error_linear <<- c(error_linear, sqrt(integrate(function(x) 
    (doppler.fun(x) - J_linear(x, f.coeff = f.coeff, j.max = j.seq[idx]) )^2, 
    lower = 0, upper = 1)$value))
    }
}


# Non linear
error_nonlinear = c()

f_nonlinear <- function(f.coeff){
  for (idx in 1:length(j.seq)){
    
    curve(doppler.fun(x), from = 0, to = 1, 
          main = paste(j.seq[idx], "-term approximation", sep = ""), xlab = "", 
          ylab = expression(m[J](x)), n = 1001, col = gray(.8), lwd = 3)
    
    curve(J_nonlinear(x, f.coeff = f.coeff, j.seq[idx]), 
          n = 1001, col = mycol[idx], lwd = 5,add = TRUE)
    
    error_nonlinear <<- c(error_nonlinear, sqrt(integrate(function(x) 
      (doppler.fun(x) - J_nonlinear(x, f.coeff = f.coeff, j.max = j.seq[idx]) )^2, 
      lower = 0, upper = 1)$value))
  }
}

f_linear(f.coeff)
f_nonlinear(f.coeff)
```

From the plots above we can clearly see that low $j$ capture the low resolution behavior of the Doppler function, while at high $j$, $\phi_j$ tends to describe the high resolution details.
We can also clearly observe the effect of the inhomogeneous smoothness of the Doppler function on the approximation. In fact, also at small values of $j$, the smoother part of the function is already well approximated, while the high complexity of the function around the origin is hardly represented.

One more detail we can appreciate from this plots is that, after a certain $j$, the addition of more values for the estimation does not improve it significantly. To justify this let's recall that the decay of the Fourier coefficients, for a given basis $\phi_j$, is related to the smoothness of the function under consideration. This implies that, if a function is smooth, its Fourier coefficients $\beta_j$ will be small. 

Here, we provide a plot to clarify this point.

```{r check, fig.width=10, fig.height=5}

plot(abs(f.coeff), ylab = 'Fourier coefficient (abs value)', 
     main='Coefficients captured by Linear Fourier Expansion')
abline(v = 5, lwd = 3, col = mycol[1])
abline(v = 10, lwd = 3, col = mycol[2])
abline(v = 25,lwd = 3,col = mycol[3])
abline(v = 50, lwd = 3, col = mycol[4])
abline(v = 100, lwd = 3,col = mycol[5])
abline(v = 150, lwd = 3, col = mycol[6])
```


### L2 -reconstruction error

```{r bonus}

error_df <- data.frame(error_linear, error_nonlinear)
rownames(error_df) <- j.seq
datatable(error_df)

```

To conclude, the choice of considering the $j$ greatest values of $\beta$ seems to be a valid choice, since the information that we are missing is probably not significantly relevant for the approximation. In order to have a better grasp on how the two approaches for approximation behave, we finally have a look at the *L2-reconstruction error* of both the linear and non-linear case.

