---
title: "HOMEWORK 01"
author: "G01: Betti, D'Arrigo, Masci, Mata Naranjo"
output: html_document
---

```{r library, echo=FALSE, message=FALSE, warning=FALSE}
require(viridis) 
library(DT)
```

<br><br>

# Part 1 - Linear Algebra is good...

<br>

### Question 3.a

<br>

We will try to explain this parallelism by first reviewing the logic used in the Transformed Feature Space.

As we have seen in class, having a finite number of feature variables (for the sake of simplicity lets make this 1 and denote it by $x$), we could use them to predict another target variable (1 dimensional, $y$) through a linear combination as follows:

<br>

\begin{align}
y = \beta_0 \cdot \beta_1x
\end{align}

<br>

Such model assumptions might not be sufficient to capture all the information in $y$ through this simple transformation (when projecting $y$ on the 1-dimensional space of $x$ we might lose too much information). For this reason, we might want to think of a more complex set of transformations we can apply on our feature $x$ in order to capture more details about the target variable. This can be accomplished through a function/transformation $\phi(x)$ such that:

<br>

\begin{align}
y = \sum_{j=1}^n \beta_j \cdot \phi_j(x)
\end{align}

<br>

This is basically the same principle used in the series expansion in which we try to expand a function $m(x)$ (possibly complex) by adding up less complex and therefore easier to handle functions together:

<br>

\begin{align}
m(x) = \sum_{j=1}^n \beta_j \cdot \phi_j(x)
\end{align}

<br>

It is important to highlight that one of the basic reasons for which these two principles work (in the vector and function space) is because in both case it is possible to exploit the dot product property between the elements living in the respective spaces.

Finally, we would also like to highlight that very nice properties pop up in both scenarios if the transformations ($\phi_j(x)$) form an orthonormal set of functions/vectors:

- Adding additional components to the sum would not change the previous $\beta_j$ values.
- $\beta_j$ can be estimated using the Generalized Fourier Expansion, which in the limit tends to an interpolation function of y or $m(x)$.

<br><br><br>

### Question 3.b/c

<br>

We know that a function $m(\cdot)$ can be approximated by less complex functions through a linear combination of Fourier coefficients. In order to find the best approximation we must define a system to work with. If we are in a Hilbert space $H$ with an orthonormal basis $\phi_j(x)$, the best approximant is the partial sums $m=\sum_{j=1}^J\langle m(x),\phi_j(x)\rangle$, where $m(x)$ is just a linear, $J$ dimensional subspace of $H$. 

However, to provide a better approximation error, it may be convenient to use a non-linear approximation of the form $m_J^*(x)$ to the Doppler function, which requires less Fourier coefficients (reducing the $||m||^2$).
For a fixed $J$, $m_J^*(x)$ has the same cardinality (and therefore complexity) as $m_J(x)$, but in the non linear case the approximants $\{\beta_j\}^J_j$ come from different linear sub-spaces.
We know that, since the *Fourier* coefficients are the optimal choice to minimize $\Delta_J=||m-\sum_{j=1}^Jc_j\phi_j||$, the best choice of $c_j$ is $\beta$. Consequently, the best choice of $\Delta_J$ is the set of the indices $J$ with the biggest coefficients in absolute value of $\beta$. 

It is important to underline that this applies only to the extremely simple case of a Hilbert space with an orthonormal base, therefore we will consider a cosine basis.

<br><br>

### Defining Cosine-basis and Doppler function scaled in [0,1]

<br>

```{r setup}
cos.basis = function(x, j) 1*(j == 0) + sqrt(2)*cos(pi*j*x)*(j > 0)
doppler.fun <-  function(x) sqrt(x*(1 - x))*sin( (2.1*pi)/(x + 0.05))
```

<br>

Now we calculate the Fourier coefficients by integrating over the interval [0,1]. 

<br>

```{r fourier coefficients}
j.max   <- 200
f.coeff <- rep(NA, j.max+1)
for (idx in 0:j.max){
  foo = tryCatch(
    integrate(function(x, j) doppler.fun(x) * cos.basis(x,j), lower = 0, upper = 1, j = idx)$value,
    error = function(e) NA
  )
  f.coeff[idx + 1] = foo
}
```

<br>

In the linear case we approximate our function by picking the first $m$ Fourier transforms. In the non-linear case we pick $m$ indices corresponding to the greater Fourier transforms in absolute values.

<br>

```{r funcs}
# sorting indexes of Fourier transform
idx_sorted <- sort(abs(f.coeff), decreasing = T, index.return = T)$ix
# Linear
J_linear <- function(x, f.coeff = f.coeff, j.max){
  out = rep(0, length(x))
  for(idx in 0:j.max){
    if(!is.na(f.coeff[idx + 1])) out = out + f.coeff[idx + 1] * cos.basis(x, j = idx)
  }
  return(out)
}
# Non linear
J_nonlinear <- function(x, f.coeff = f.coeff, j.max){
  out = rep(0, length(x))
  for(idx in head(idx_sorted,j.max)){
    if(!is.na(f.coeff[idx])) out = out + f.coeff[idx] * cos.basis(x, j = idx-1)
  }
  return(out)
}
```

<br>

Lastly we visualize some n-terms approximations of the Doppler function in the linear and non-linear case.

<br>

```{r plots, fig.width=18, fig.height=10}
par(mfrow = c(2,3))
j.seq = c(5, 10, 25, 50, 100, 150)
mycol = viridis(length(j.seq), alpha = .7)
# Linear
error_linear = c()
f_linear <- function(f.coeff){
  for (idx in 1:length(j.seq)){
     
    curve(doppler.fun(x), from = 0, to = 1, 
          main = paste(j.seq[idx], "-term approximation", sep = ""), xlab = "", 
          ylab = expression(m[J](x)), n = 1001, col = gray(.8), lwd = 3)
    
    curve(J_linear(x, f.coeff = f.coeff , j.seq[idx]),
          n = 1001, col = mycol[idx], lwd = 5,add = TRUE)
    error_linear <<- c(error_linear, sqrt(integrate(function(x) 
    (doppler.fun(x) - J_linear(x, f.coeff = f.coeff, j.max = j.seq[idx]) )^2, 
    lower = 0, upper = 1)$value))
    }
}
# Non linear
error_nonlinear = c()
f_nonlinear <- function(f.coeff){
  for (idx in 1:length(j.seq)){
    
    curve(doppler.fun(x), from = 0, to = 1, 
          main = paste(j.seq[idx], "-term approximation", sep = ""), xlab = "", 
          ylab = expression(m[J](x)), n = 1001, col = gray(.8), lwd = 3)
    
    curve(J_nonlinear(x, f.coeff = f.coeff, j.seq[idx]), 
          n = 1001, col = mycol[idx], lwd = 5,add = TRUE)
    
    error_nonlinear <<- c(error_nonlinear, sqrt(integrate(function(x) 
      (doppler.fun(x) - J_nonlinear(x, f.coeff = f.coeff, j.max = j.seq[idx]) )^2, 
      lower = 0, upper = 1)$value))
  }
}
f_linear(f.coeff)
f_nonlinear(f.coeff)
```

<br>

From the plots above we can clearly see that low $j$ capture the low resolution behavior of the Doppler function, while at high $j$, $\phi_j$ tends to describe the high resolution details.
We can also clearly observe the effect of the inhomogeneous smoothness of the Doppler function on the approximation. In fact, also at small values of $j$, the smoother part of the function is already well approximated, while the high complexity of the function around the origin is hardly represented.

One more detail we can appreciate from this plots is that, after a certain $j$, the addition of more values for the estimation does not improve it significantly. To justify this let's recall that the decay of the Fourier coefficients, for a given basis $\phi_j$, is related to the smoothness of the function under consideration. This implies that, if a function is smooth, its Fourier coefficients $\beta_j$ will be small. 

Here, we provide a plot to clarify this point.

<br>

```{r check, fig.width=10, fig.height=6, fig.align='center'}
plot(abs(f.coeff), ylab = 'Fourier coefficient (abs value)', 
     main='Coefficients captured by Linear Fourier Expansion')
abline(v = 5, lwd = 3, col = mycol[1])
abline(v = 10, lwd = 3, col = mycol[2])
abline(v = 25,lwd = 3,col = mycol[3])
abline(v = 50, lwd = 3, col = mycol[4])
abline(v = 100, lwd = 3,col = mycol[5])
abline(v = 150, lwd = 3, col = mycol[6])
```

<br><br>

### L2 -reconstruction error

<br>

```{r bonus}
error_df <- data.frame(error_linear, error_nonlinear)
rownames(error_df) <- j.seq
datatable(error_df)
```

To conclude, the choice of considering the $j$ greatest values of $\beta$ seems to be a valid one, since the information that we are missing is probably not significantly relevant for the approximation. In order to have a better grasp on how the two approaches for approximation behave, we finally have a look at the *L2-reconstruction error* of both the linear and non-linear case.

<br>
---
<br>

## Part 2: Polynomials are good...

<br>

### Question 1

<br>

As briefly explained at the beginning of this report, linear combinations of features in a transformed space allow to address the non-linearity of the data in the original space, defining powerful and flexible yet relatively simple models. Recall, once again, that a linear model is defined as

<br>

\begin{equation}
    y = \sum_{j=1}^n \beta_j \cdot \phi_j(x) \hspace{1cm} (1)
\end{equation}

<br>

for certain $\phi_j : \mathbb{R}^p \mapsto \mathbb{R}$ transformations. Polynomial regression, for instance, involves $\phi_j$ such that $\phi_j(x)=x^d$, for $d\in \{0,1,...,D\}$. Even though it provides a flexible representation for $y$, it is susceptible to data point variations and may incur in overfitting; moreover, it is inherently non-local, i.e. the value of $y$ at a certain point may affect the fit for data points very far away. In order to overcome these drawbacks, the interval the data belong to is divided into subintervals $(-\infty,\xi_1],[\xi_1,\xi_2],[\xi_2,\xi_3],...,[\xi_q,+\infty)$ and a polynomial is fitted in each of them: this is the roughly and high-level idea behind splines. $\{\xi_i\}_{i=1}^q$ are known as knots. Obviously, some additional constraints are required: degree-$d$ splines must be continuous and must have continuous derivatives at the knots up to $d-1$. This way, the overall obtained curve is well-shaped and continuous at each point and, by reducing the number of degrees of freedom, the complexity of the problem is also reduced. Finally, splines are such that

<br>

$$
    f(x) = \sum_{j=1}^{(d+1)+q} \beta_j \cdot g_j(x),
$$

<br>

\noindent having $\mathcal{G}_{d,q}=\big\{g_1(x), ..., g_{d+1}(x), g_{(d+1)+1}(x), ..., g_{(d+1)+q}(x)\big\}$ the set of the $g_j$ transformation functions on the input data $x$ so that 

$$\big\{g_1(x)=1,g_2(x)=x,...,g_{d+1}(x)=x^d\big\}$$
and 

$$\big\{g_{(d+1)+j}(x)=(x-\xi_j)_+^d\big\}_{j=1}^q$$
Again, linearity in transformed feature space pops up.
Pushing the analysis a step further, it is worth to mention that *(1)* is a linear basis expansion in $x$. Then, we may notice that both splines and series expansion exploit the same notion: linear basis. Although the solution the two methods come up with is somewhat similar, the difference lies in the way it is reached. Indeed, splines divide the interval, generically say $[a,b]$, into subintervals and the approximation of the (unknown) data generating function $m(\cdot)$ is tuned by changing the number of knots, i.e. analyzing more locally the behavior of the function, and by changing the degree $d$ of the polynomial, i.e. changing the basis; on the other hand, series expansions look at the whole interval $[a,b]$ and control the approximation by varying the number $J$ of terms after which the series is truncated; they leverage the properties of the Hilbert space.

<br><br>

### Question 2

<br>

We plot some of the elements of $\mathcal{G}_{d,q}$, given by the formula:
\begin{align}
\mathcal{G}_{d,q} = \{g_1(x), ..., g_{d+1}(x), g_{(d+1)+1}(x), ..., g_{(d+1)+q}(x)\}
\end{align}
where:
\begin{align} \{g_1(x) = 1, g_2(x) = x, ..., g_{d+1}(x) = x^d\} \end{align}
\begin{align} \{g_{(d+1)+j}(x) = (x - \xi_j)_+^d\}_{j=1}^q \end{align}

The elements we want to plot are those having $q\in\{1,3,5\}$ and $d\in\{3,5,10\}$, with equispaced knots in the interval $[0,1]$. The visualized elements are chosen randomly.

<br>

```{r include=FALSE, warning=FALSE, message=FALSE}
library(latex2exp)
set.seed(123)
par(mfrow = c(1,1))
```

```{r spline_plots, echo=FALSE, fig.width=10, fig.height=8, fig.align='center'}
g <- function(q=3, d=1, n=3){
  knots <- seq(0,1, by =  1/(q+1))
  knots <- knots[1:q+1]
  mycol <- viridis(length(knots))
  
  card_G <- d+q+1
  chosen_funcs <- sample(x = 1:card_G, replace = FALSE, size = n)
  
  #plot of each chose function
  j <- 0
  for (i in chosen_funcs) {
    j <- j + 1
    if (i <= d + 1){
      if (j == 1) {
        curve(x^(i-1), from = 0, to = 1, col = mycol[j], ylim = (0:1), xlim = c(0, 1.05), 
              lwd = 4, main = TeX("Some elements of \\mathit{G}_{d,q}"),
              xlab = "Basis", ylab = "y")
        text(1.04, (1)^(i-1), labels = TeX(paste0("$x^{",i-1,"}$")))
      } else {
        curve(x^(i-1), from = 0, to = 1, add = TRUE, col = mycol[j], lwd = 4)
        text(1.04, (1)^(i-1), labels = TeX(paste0("$x^{",i-1,"}$")))
      }
      
    } else {
      if (j == 1) {
        curve(((x-knots[i-q])^(d)), from = 0, to = 1, col = mycol[j], ylim = (0:1), xlim = c(0, 1.05), 
              lwd = 4, main = TeX("Some elements of \\mathit{G}_{d,q}"),
              xlab = "Basis", ylab = "y")
        text(1.04, (1-knots[i-q])^(d), labels = TeX(paste0("$(x-\\xi_{",i-q,"})^",d,"$")))
      } else {
        curve((x-knots[i-q])^(d), from = 0, to = 1, add = TRUE, col = mycol[j], lwd = 4)
        text(1.04, (1-knots[i-q])^(d), labels = TeX(paste0("$(x-\\xi_{",i-q,"})^",d,"$")))
      }
    }
  }
}
```

The function takes as input the number of knots, the order of the power functions and the number of total functions we want to show in the final graph. In the following example, we see 4 random functions belonging to the family of functions defined above, with $q=4$ and $d=3$:

```{r}
g(4,3,4)
```

<br>

having $\xi_1=0.2$, $\xi_2=0.4$, $\xi_3=0.6$, $\xi_4=0.8$.

<br><br>

### Question 3

<br>

At this point of the analysis, we implement the splines' model and we apply it to the data `ieri_domani`. First, we load the data:

<br>

```{r load_data, message=FALSE, warning=FALSE}
# ieri_domani -------------------------------------------------------------
load(".../data/ieri_domani.RData")
head(data.frame(x.train = df$x, y.train = df$y.yesterday, x.test = df$x, y.test=df$y.tomorrow))
# Train/Test
train <- data.frame(x = df$x, y = df$y.yesterday)
test  <- data.frame(x = df$x, y = df$y.tomorrow)
```

<br>

Recalling the definition of splines provided above, a crucial step is to build the design matrix $\mathbb{X}$ which we are going to work on. Formally, it is defined as:

$$
\mathbb{X}_{i,j} = g_j(x_i) \hspace{5mm} \text{for} \hspace{2mm} i\in\{1,...n\} \hspace{2mm} \text{and} \hspace{2mm} j\in\{1,...,(d+1)+q\}
$$

We may notice that, in defining the splines, just the set of functions $\big\{g_{(d+1)+j}(x)=(x-\xi_j)_+^d\big\}_{j=1}^q$ depends on the number of knots $q$, while the set $\big\{g_1(x)=1,g_2(x)=x,...,g_{d+1}(x)=x^d\big\}$ is fixed once $d$ is fixed. Hence, having $d=3$, a function can compute the general matrix on the common basis, leaving to another function the task of computing the variable part, as in the following:

<br>

```{r design_mat}
# First step is to generate the 3 functions we will be fitting our training data over
# For this purpose we are given 2 parameters in each case: (d=3, q=3), (d=3, q=5), 
# (d=3, q=10)
q_vals = c(3, 5, 10)
d = 3
# Generate the part of the general design matrix that is common to all
generate_gdm_func <- function(train) {
  general_design_matrix = matrix(NA, nrow = nrow(train), ncol = (d+1))
  general_design_matrix[,1] = 1
  general_design_matrix[,2] = train$x
  general_design_matrix[,3] = (train$x)^2
  general_design_matrix[,4] = (train$x)^3
  return(general_design_matrix)
}
create_matrix <- function(q, train) {
  design_matrix = matrix(NA, nrow = nrow(train), ncol = q)
  knots = quantile(train$x, probs = seq(0, 1, length.out = q+2))[1:q+1]
  for (i in 1:q) {
    design_matrix[,i] = (pmax(0, train$x-knots[i]))^3
  }
  return (design_matrix)
}
```


<br>

By means of the functions below, the values of the coefficients $\{\beta\}_j$ are determined and the fitted regression splines $\hat{f}_j$ are obtained:

<br>

```{r fhat}
fhat_func <- function(q, train_coef=train, predict_sample=train) {
  y = train$y
  beta = lm(y ~ generate_gdm_func(train)[,2:4] + create_matrix(q, train))
  fhat <- predict(beta, newdata = list(x = predict_sample$x))
  return ('fhat' = fhat)
}
mycol <- viridis(3)
fhat_plot <- function(q, train, i) {
  y = train$y
  fhat <- fhat_func(q, train)
  plot( y ~ x , train, lwd = 2, cex = .5, main = paste("Degree-3 spline with q=", q, sep=""))
  lines(fhat ~ x, train, lwd = 4, col = mycol[i])
}
```

<br>

Before going any further, it is worth to have a visual evaluation of the spline models as the number of knots $q$ varies in the set $\{3,5,10\}$:

<br>

```{r fhat_plots, echo=FALSE, warning=FALSE, message=FALSE, fig.width=10, fig.height=10, fig.align='center'}
par(mfrow=c(3,1))
fhat_plot(3, train,1)
fhat_plot(5, train,2)
fhat_plot(10, train,3)
```

<br>

As the plots show, we may, at this point qualitatively, notice that as the number of knots increases, the fitted curve locally changes more rapidly, i.e. the variance of the model rises. It seems evident, for instance, that having $q=10$ leads to a model that overfits the data. <br> Nevertheless, a numeric evaluation is needed and thus we move forward analyzing first the $C_p$ of the models. 

<br>

```{r cp, fig.width=10, fig.height=6, fig.align='center'}
mean_squared_error <- function(y_pred, f_hat) mean((y_pred-f_hat)^2)
q_3 <- fhat_func(3, train_coef=train, predict_sample=train)
q_5 <- fhat_func(5, train_coef=train, predict_sample=train)
q_10 <-fhat_func(10, train_coef=train, predict_sample=train)
fhats = list(q_3, q_5, q_10)
cp = rep(NA, 3)
for (i in 1:3) {
  MSE_val = mean_squared_error(y_pred = train$y, f_hat = fhats[[i]])
  p = 4+q_vals[i]
  n = length(train$y)
  sigma_hat = sum((train$y - fhats[[i]])^2)/(n-p)
  second_term = 2*sigma_hat*(p/n)
  cp[i] = MSE_val + second_term
}
plot(cp, type = 'b', lwd = 2, col = rgb(.1,.1,.1, .4), main = expression(paste("C"[p], " of the models")), 
     xlab = "Model", ylab = expression(C[p]), xaxt = "none", xlim=c(1, 3.2))
for(i in 1:3) text(i+.1, cp[i], labels = paste("q=", q_vals[i], sep = ""))
```

<br>

It is important to bare in mind that, since $C_p = MSE_{Tr} + \frac{2\cdot\hat{\sigma}^2\cdot p}{n}$ looks at the Mean Squared Error on the training data, it leads to a large overoptimism: against evidence, the model having $q=10$ has the best $C_p$ value, as foreseeable.<br> Thus, cross-validation is a more reliable technique to select the optimal $q$ value; among the different `CV` flavors, `5-fold CV` and `LOOCV` have been selected for the purpose:

<br>

```{r include=FALSE}
set.seed(123) # for reproducibility
```

```{r fig.width=10, fig.height=7, fig.align='center'}
k_fold_cv <- function(q, train, K=5, predictor=fhat_func) {
  n = length(train$y)
  folds = sample(rep(1:K, length=n))
  kcv = sapply(1:K, function(k) {
    train_set = train[which(folds != k), ]
    test_set = train[which(folds == k), ]
    return(mean_squared_error(y_pred = test_set$y, 
                              f_hat = predictor(q, train_coef = train_set, 
                                                predict_sample = test_set)))
  })
  return(mean(kcv))
}
cv <- sapply(q_vals, k_fold_cv, train=train)
loocv <- sapply(q_vals, k_fold_cv, train=train, K=length(train$y))
cols <- c(rgb(.2, .05, .6, .5), rgb(.4, .2, .1, .5))
plot(cv, lwd = 3, type = "b", col = cols[1], main = "CV and LOOCV of the models", 
     xlab = "Model", ylab = "CV", xlim = c(1,3.2), xaxt = "none")
for(i in 1:3) text(i+.1, cv[i], labels = paste("q=", q_vals[i], sep = ""))
lines(loocv, lwd = 3, type = "b", col = cols[2])
for(i in 1:3) text(i+.1, loocv[i], labels = paste("q=", q_vals[i], sep = ""))
legend("topleft", legend = c("5-fold cross-validation", "leave-one-out cv"), 
       col = cols, lty = 1, lwd = 3, cex = .8)
```

<br>

The values of the `5-fold CV` and `LOOCV` reward $q=3$, which is selected as the optimal value for $q$. Eventually, the best model on the data is $\hat{f}_{spline}(x; d=3, q=3)$.

<br><br>

### Question 4

```{r original_poly, include=FALSE}
set.seed(123)
# poly fit
poly_func <- function(d, train_coef=train, predict_sample=train) {
  if (d == 0) beta = lm(y ~ 1, train) 
  else beta = lm(y ~ poly(x, degree = d), train)
  fhat <- predict(beta, newdata = list(x = predict_sample$x))
  return('fhat' = fhat)
}
ftrue <- c(0.4342,0.4780,0.5072,0.5258,0.5369,0.5426,0.5447,0.5444,0.5425,0.5397,
0.5364,0.5329,0.5294,0.5260,0.5229,0.5200,0.5174,0.5151,0.5131,0.5113,
0.5097,0.5083,0.5071,0.5061,0.5052,0.5044,0.5037,0.5032,0.5027,0.5023)
```

<br>

In the end, we compare the best spline model $\hat{f}_{spline}(x; d=3, q=3)$ with the polynomial regression model $\hat{f}_{poly}(x; d=3)$ of degree 3 and the true data generating function.

<br>

```{r final_plot, fig.width=10, fig.height=6, echo=FALSE, fig.align='center'}
fhat_plot(q = 3, train, 1)
lines(poly_func(d = 3) ~ x, train, lwd = 4, col = mycol[2])
lines(train$x, ftrue, lwd = 4, lty = 2, col = mycol[3])
legend("bottomright", legend = c("degree-3 regression spline", 
                                 "degree-3 polynomial regression", 
                                 "true function"), 
       col = mycol, lty = c(1,1,2), lwd = 3, cex = .8)
```

<br>

The degree-3 polynomial regression model shows a lower capacity than the spline, even though they share the same degree $d$. Thus, the advantages of the spline mentioned earlier are evident: the spline locally fits the data better, without the need of increasing the complexity of the polynomial. Quantitatively, the $MSE_{Te}$ of the spline is $MSE_{Te}(\hat{f}_{spline})=$ 
`r options(scipen=10); mean_squared_error(test$y, fhat_func(3, predict_sample=test))`, whereas the one of the polynomial predictor is $MSE_{Te}(\hat{f}_{poly})=$ 
`r options(scipen=10); mean_squared_error(test$y, poly_func(3, predict_sample=test))`.

<br><br>